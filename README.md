
ğŸ¡ California Housing Price Prediction (Linear Regression from Scratch)

This project implements Linear Regression from scratch to predict house prices using the California Housing dataset.
Two training approaches are compared:
	â€¢	Closed-form Normal Equation
	â€¢	Stochastic Gradient Descent (SGD) with learning-rate decay

The goal is to understand optimization behavior, convergence, and generalization without relying on high-level ML libraries.


ğŸ“Œ Project Overview
	â€¢	Dataset: California Housing (Scikit-learn)
	â€¢	Task: Regression
	â€¢	Target Variable: Median house value
	â€¢	Models Implemented:
	â€¢	Linear Regression via Normal Equation
	â€¢	Linear Regression via Stochastic Gradient Descent



ğŸ§  Key Concepts Covered
	â€¢	Feature normalization (z-score standardization)
	â€¢	Bias term handling
	â€¢	Train / Validation / Test split (60% / 20% / 20%)
	â€¢	Mean Squared Error (MSE) evaluation
	â€¢	Learning rate decay
	â€¢	Convergence analysis using loss curves



ğŸ› ï¸ Tech Stack
	â€¢	Python
	â€¢	NumPy â€“ numerical computation
	â€¢	Matplotlib â€“ loss visualization
	â€¢	Scikit-learn â€“ dataset loading only (no models used)



âš™ï¸ Implementation Details

1ï¸âƒ£ Data Preprocessing
	â€¢	Loaded California Housing dataset
	â€¢	Normalized all features using z-score normalization
	â€¢	Added a bias (intercept) term manually

2ï¸âƒ£ Data Splitting
	â€¢	60% Training
	â€¢	20% Validation
	â€¢	20% Testing
(Randomized using fixed seed for reproducibility)


ğŸ“ Models Implemented

ğŸ”¹ Normal Equation

A closed-form solution:

\theta = (X^TX)^{-1}X^Ty
	â€¢	Fast convergence
	â€¢	Requires matrix inversion
	â€¢	Suitable for smaller datasets



ğŸ”¹ Stochastic Gradient Descent (SGD)
	â€¢	Updates parameters one sample at a time
	â€¢	Includes learning rate decay
	â€¢	Tracks training and validation MSE per epoch

Î¸ â† Î¸ âˆ’ Î± (Å· âˆ’ y) x




ğŸ“Š Evaluation Metric
	â€¢	Mean Squared Error (MSE)

Computed separately for:
	â€¢	Training set
	â€¢	Validation set
	â€¢	Test set



ğŸ“ˆ Results & Observations
	â€¢	Normal Equation achieves stable performance quickly
	â€¢	SGD converges gradually with proper learning rate tuning
	â€¢	Validation loss helps monitor overfitting
	â€¢	Learning rate decay improves SGD stability



ğŸ“‰ Visualization
	â€¢	Training vs Validation loss curves
	â€¢	Clear convergence behavior for SGD
	â€¢	Helps diagnose underfitting / overfitting



ğŸ¯ Learning Outcomes
	â€¢	Gained deep understanding of linear regression internals
	â€¢	Implemented optimization without ML libraries
	â€¢	Learned trade-offs between analytical and iterative solutions
	â€¢	Practiced proper ML evaluation workflows



ğŸ”® Future Improvements
	â€¢	Add Mini-Batch Gradient Descent
	â€¢	Compare with Ridge / Lasso Regression
	â€¢	Add RÂ² score evaluation
	â€¢	Hyperparameter tuning for learning rate



ğŸ‘¤ Author

Bhavesh Maurya
Machine Learning & Data Science Enthusiast

